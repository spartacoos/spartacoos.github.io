
#+BEGIN_EXPORT html
<div class="neon">
#+END_EXPORT

#+TITLE: 27 Days of JAX!
#+OPTIONS: toc:t num:nil html-style:nil tex:t

# #+LATEX_CLASS: article
# #+LATEX_CLASS_OPTIONS: [a4paper]
# #+LATEX_HEADER: \usepackage{times}

#+LATEX_HEADER: \usepackage[parfill]{parskip}
# +HTML: Literal HTML code for export

I‚Äôve been hearing all this talk in ML circles about how **Differentiable Programming** is supposed to be the next big thing that will supersede our current workflows. Still, me being the sceptic that I am, I stayed away from it since, from what I understood, it‚Äôs meant to be mainly a performance boost for matrix computations which can speed up training, but since this isn‚Äôt critical to my projects or research, it didn‚Äôt seem worth the hassle. Besides, these days it feels like every new paradigm is supposedly able to do magic, at least according to those who evangelize it. However, after letting the curious side of me look into some of the code examples for Taichi (a differentiable programming language compatible with Python), it seemed like there were additional benefits to this technology other than just raw performance increase. Then yesterday, I stumbled upon this tweet:

#+HTML: <blockquote class="twitter-tweet"><p lang="en" dir="ltr">In an effort to invite everyone to spend more time playing w JAX:<br><br>Announcing <a href="https://twitter.com/hashtag/27DaysOfJAX?src=hash&amp;ref_src=twsrc%5Etfw">#27DaysOfJAX</a>! üèéüî•<br><br>Do:<br><br>- üìö Spend some time w JAX<br>- ‚úçÔ∏è Blog about it or share your code ex with us<br>- üé§ Tag <a href="https://twitter.com/weights_biases?ref_src=twsrc%5Etfw">@weights_biases</a>, <a href="https://twitter.com/cgarciae88?ref_src=twsrc%5Etfw">@cgarciae88</a>, or myself<br><br>Best works will receive some W&amp;B Swag (&amp; more)! üçµ</p>&mdash; Sanyam Bhutani (@bhutanisanyam1) <a href="https://twitter.com/bhutanisanyam1/status/1465650810474287112?ref_src=twsrc%5Etfw">November 30, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

So, I decided to figure out what differentiable programming is all about and whether its benefits are good (for my use cases) and if there are any drawbacks (I assume many apriori).
Thus I'm starting a blog mini-series where I document my journey with JAX.

#+BEGIN_QUOTE
ML workloads often consist of large, accelerable, pure-and statically-composed subroutines orchestrated by dynamic
logic. - Matthew Johnson - [Google Brain]
#+END_QUOTE

# +begin_src latex :results drawer :exports results
# \[  \frac{1}{2} \]
# +end_src

* Day 1: Whetting our apetite for performance!
First, we want to activate our conda environment and then install jax:
#+BEGIN_EXAMPLE
conda install -c conda-forge jax
#+END_EXAMPLE
Now, let's see what is the performance compared to numpy on a simple task:
#+HTML: <img src="./carbon.svg" alt="Carbon code" />

# \begin{equation} % Creates an equation environment and is compiled as math
#    \gamma^2+\theta^2=\omega^2
# \end{equation}
# Normal text here:
# Discovered in 1905 by Albert Einstein. In natural units ($c$ = 1), the formula expresses the identity
Okay! It seems like that part is clear enough; JAX is straight-up blowing regular old Numpy out of the water.

**Tomorrow**: how to vectorize scalar operations?

#+BEGIN_EXPORT html
</div>
#+END_EXPORT
